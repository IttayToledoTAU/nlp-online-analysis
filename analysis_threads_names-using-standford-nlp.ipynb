{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bidi.algorithm import get_display\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from os.path import basename\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import COMMASPACE, formatdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully sent the mail\n"
     ]
    }
   ],
   "source": [
    "send_email()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(user='dsakaidf@gmail.com', pwd='d54k4idf!', recipient='shkasta@post.bgu.ac.il',\n",
    "               subject='finish expirement', body='my password is d54k4idf!'):\n",
    "    import smtplib\n",
    "\n",
    "    gmail_user = user\n",
    "    gmail_pwd = pwd\n",
    "    FROM = user\n",
    "    TO = recipient if type(recipient) is list else [recipient]\n",
    "    SUBJECT = subject\n",
    "    TEXT = body\n",
    "\n",
    "    # Prepare actual message\n",
    "    message = \"\"\"From: %s\\nTo: %s\\nSubject: %s\\n\\n%s\n",
    "    \"\"\" % (FROM, \", \".join(TO), SUBJECT, TEXT)\n",
    "    try:\n",
    "        # SMTP_SSL Example\n",
    "        server_ssl = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465)\n",
    "        server_ssl.ehlo()  # optional, called by login()\n",
    "        server_ssl.login(gmail_user, gmail_pwd)\n",
    "        # ssl server doesn't support or need tls, so don't call server_ssl.starttls()\n",
    "        server_ssl.sendmail(FROM, TO, message)\n",
    "        # server_ssl.quit()\n",
    "        server_ssl.close()\n",
    "        print('successfully sent the mail')\n",
    "    except:\n",
    "        print(\"failed to send mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_nlp = stanza.Pipeline('he', processors='tokenize,mwt', verbose=False, use_gpu=False)\n",
    "#he_nlp = stanza.Pipeline('he', processors='tokenize,lemma,pos', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 1.10MB/s]\n",
      "2020-04-28 23:41:17 INFO: Downloading default packages for language: he (Hebrew)...\n",
      "2020-04-28 23:41:17 INFO: File exists: C:\\Users\\Michael\\stanza_resources\\he\\default.zip.\n",
      "2020-04-28 23:41:20 INFO: Finished downloading models and saved to C:\\Users\\Michael\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('he', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_file = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = 'message'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.listdir('data/')\n",
    "for filename in file_name:\n",
    "    if type_file in filename:\n",
    "        temp_df = pd.read_csv(f'data/{filename}')\n",
    "        if threads_df is None:\n",
    "            threads_df = temp_df\n",
    "        else:\n",
    "            threads_df = threads_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = threads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>thread_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>cite1</th>\n",
       "      <th>cite2</th>\n",
       "      <th>cite3</th>\n",
       "      <th>cite4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834374</td>\n",
       "      <td>רומא</td>\n",
       "      <td>21-02-2020 20:54</td>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834407</td>\n",
       "      <td>רומא</td>\n",
       "      <td>21-02-2020 20:54</td>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834485</td>\n",
       "      <td>רומא</td>\n",
       "      <td>21-02-2020 20:54</td>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834494</td>\n",
       "      <td>רומא</td>\n",
       "      <td>21-02-2020 20:54</td>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20126916</td>\n",
       "      <td>post_202834534</td>\n",
       "      <td>רומא</td>\n",
       "      <td>21-02-2020 20:54</td>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>login.php?do=lostpw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  thread_id         post_id user_name              date  \\\n",
       "0           0   20126916  post_202834374      רומא  21-02-2020 20:54   \n",
       "1           1   20126916  post_202834407      רומא  21-02-2020 20:54   \n",
       "2           2   20126916  post_202834485      רומא  21-02-2020 20:54   \n",
       "3           3   20126916  post_202834494      רומא  21-02-2020 20:54   \n",
       "4           4   20126916  post_202834534      רומא  21-02-2020 20:54   \n",
       "\n",
       "                                             message                cite1  \\\n",
       "0  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...  login.php?do=lostpw   \n",
       "1  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...  login.php?do=lostpw   \n",
       "2  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...  login.php?do=lostpw   \n",
       "3  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...  login.php?do=lostpw   \n",
       "4  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...  login.php?do=lostpw   \n",
       "\n",
       "                 cite2                cite3 cite4  \n",
       "0  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "1  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "2  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "3  login.php?do=lostpw  login.php?do=lostpw   NaN  \n",
       "4  login.php?do=lostpw  login.php?do=lostpw   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  word_count\n",
       "0  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...          35\n",
       "1  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...          35\n",
       "2  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...          35\n",
       "3  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...          35\n",
       "4  \\nיש כאלה בכלל?\\n \\n \\n \\n טיס עד לגיבוש  \\nשח...          35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch wordcount for each abstract\n",
    "dataset['word_count'] = dataset[field_name].apply(lambda x: len(str(x).split(\" \")))\n",
    "dataset[[field_name,'word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    334819.000000\n",
       "mean        224.657269\n",
       "std         177.378942\n",
       "min           1.000000\n",
       "25%         110.000000\n",
       "50%         186.000000\n",
       "75%         290.000000\n",
       "max        5242.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "לא     1935550\n",
       "זה     1462097\n",
       "אני    1037347\n",
       "את      939085\n",
       "אתה     778459\n",
       "על      706923\n",
       "מה      686145\n",
       "אם      668095\n",
       "לי      657111\n",
       "אבל     585003\n",
       "יש      560402\n",
       "לך      558872\n",
       "של      548140\n",
       "עם      464913\n",
       "או      434318\n",
       "גם      405743\n",
       "כל      381187\n",
       "אז      369157\n",
       "הוא     299469\n",
       "כי      297944\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(dataset[field_name]).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a list of stop words and adding custom stopwords\n",
    "import codecs\n",
    "with codecs.open('stopwords.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "stop_words = set(word for word in text.split('\\r\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "🤭🙅*♂\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "🤨זה\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "אחשלייייייייייייי⁦✌️⁩\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "🤨שקט\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "👨*🦰\n",
      "🦹*♂\n",
      "👨*🦰\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "יורד😄לי🥯על\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "רומנטי⁦\n",
      "רומנטי⁦\n",
      "רומנטי⁦\n",
      "רומנטי⁦\n",
      "רומנטי⁦\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🥰🥰\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🤫🤫🤫🤫🤫\n",
      "🥺🥺\n",
      "🥺🥺\n",
      "🥺🥺\n",
      "🥺🥺\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "⁦♥️⁩⁦♥\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "👶🏻⁩\n",
      "⁦❤️⁩\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "גולני🤮\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "⁦❤️⁩\n",
      "הפ⁦👍🏻\n",
      "הפ⁦👍🏻\n",
      "הפ⁦👍🏻\n",
      "הפ⁦👍🏻\n",
      "הפ⁦👍🏻\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤨🤨🤨🤨\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n",
      "🤯🤔\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "count = 0\n",
    "for i in dataset[field_name]:\n",
    "    text = []\n",
    "    try:        \n",
    "        he_doc = he_nlp(re.sub('- FXP', '', i))\n",
    "    except:\n",
    "        count += 1\n",
    "        continue\n",
    "    for i, sent in enumerate(he_doc.sentences):\n",
    "        for word in reversed(sent.words):\n",
    "            word_text = word.text\n",
    "            if not '_' in word_text and not word_text in stop_words and len(word_text) > 1:\n",
    "                try:\n",
    "                    text.append(get_display(word_text))\n",
    "                except:\n",
    "                    print(word.text)\n",
    "    if len(text) > 0:\n",
    "        corpus.append(' '.join(text))\n",
    "print(f'number remove setence {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "wordcloud = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          max_words=100,\n",
    "                          max_font_size=50, \n",
    "                          random_state=42,\n",
    "                          font_path='C:/WINDOWS/Fonts/Arial.ttf'\n",
    "                         ).generate(' '.join(corpus))\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "fig.savefig(\"word1.png\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = list()\n",
    "for sentence in corpus:\n",
    "    new_corpus.append(' '.join([word for word in sentence.split() if not word in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "#Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Bi-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "print(top2_df)\n",
    "#Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)\n",
    "#Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "doc=corpus[532]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\nAbstract:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
